{"name":"Ruby-spark","tagline":"Ruby wrapper for Apache Spark","body":"In this post, I describe how to parallelize computations in Ruby with **ruby-spark** gem. This library uses a Apache Spark project to storing and distributing data collections across cluster.\r\n\r\nRequirments:\r\n- Java 7+\r\n- Ruby 2+\r\n- MRI or JRuby\r\n\r\nGlossary:\r\n- Context: entry point for using Spark functionality\r\n- RDD: Resilient Distributed Dataset\r\n- Driver: a driver Spark instance (exist only once)\r\n- Executor: worker instance\r\n\r\n![Apache Spark cluster](http://spark.apache.org/docs/latest/img/cluster-overview.png)\r\n\r\n## Installation\r\n\r\n```bash\r\n# Install gem\r\ngem install ruby-spark\r\n\r\n# Build Spark and extensions (could take a while)\r\nruby-spark build\r\n\r\n# Set JAVA_HOME (required for MRI)\r\nexport JAVA_HOME=\"...\"\r\n```\r\n\r\n## Starting and configurations\r\n\r\nFor all setup options, please look on [wiki](https://github.com/ondra-m/ruby-spark/wiki/Configuration). All necessary configuration are set by default but if you want change it you need set keys before creating context. After that is configuration read-only.\r\n\r\n```ruby\r\nrequire 'ruby-spark'\r\n\r\n# Configuration\r\nSpark.config do\r\n  set_app_name 'My RubySpark'\r\n  set_master   'local[*]'\r\n  set 'spark.ruby.batch_size', 2048\r\nend\r\n\r\n# Create a context\r\nSpark.start\r\n\r\n# Context reference\r\nsc = Spark.sc\r\n```\r\n\r\nYou can also start prepared console by `ruby-spark shell`. This command will load RubySpark and create Pry console.\r\n\r\n## Usage\r\n\r\nFirst, you need create a distributed data collection. This dataset will be splitted into computing process. All process have the same computing function and cannot comunicate with each other.\r\n\r\n```ruby\r\nworker_nums = 2\r\nrands = Array.new(1000){ rand(1..10) }\r\n\r\nrdd_numbers = sc.parallelize(1..1000, worker_nums)\r\nrdd_rands = sc.parallelize(rands, worker_nums)\r\ntext_file = sc.text_file('/etc/hosts', worker_nums)\r\n```\r\n\r\nNow you can define a computing function. All function can be found at [ruby-doc](http://www.rubydoc.info/github/ondra-m/ruby-spark/Spark/RDD). Every new function is attached to dataset and are executed at once by `.collect` (lazy definition). However some methods start calculation automatically (e.g. sum, count, first, ...).\r\n\r\n```ruby\r\n# Simple map\r\nrdd = rdd_numbers.map(lambda{|x| x*2}).collect # => [2, 4, 6, 8, 10, 12, ...]\r\n\r\n# Pipelined methods\r\nfunc = lambda do |iterator|\r\n  iterator.map { |(modulo, max)|\r\n    \"Modulo: #{modulo} have max #{max}.\"\r\n  }\r\nend\r\n\r\nrdd = rdd_rands.map(:to_f)          # `.to_f` will be called on ever item\r\nrdd = rdd.filter(lambda{|x| x > 2}) # select items greater than 2\r\nrdd = rdd.group_by(lambda{|x| x%2}) # group items based on modulo\r\nrdd = rdd.map_values(:max)          # get max from values\r\nrdd = rdd.map_partitions(func)      # map on whole iterator\r\nrdd.collect # => [\"Modulo: 1.0 have max 7.0.\", \"Modulo: 0.0 have max 8.0.\"]\r\n\r\n# Word count\r\nwords = text_file.flat_map(:split)\r\narrays = words.map(lambda{|word| [word, 1]})\r\ncount = arrays.reduce_by_key(lambda{|a, b| a+b})\r\ncount.collect # => [[\"127.0.0.1\", 1], [\"localhost\", 1], [\"#\", 3], ...]\r\n\r\n# PI\r\nrdd = sc.parallelize([10_000], 1)\r\nrdd = rdd.add_library('bigdecimal/math')\r\nrdd = rdd.map(lambda{|x| BigMath.PI(x)})\r\nrdd.collect # => #<BigDecimal, '0.31415926...'>\r\n\r\n# Stats\r\nrdd = rdd_numbers.map(lambda{|x| (x * rand) ** 2})\r\nstats = rdd.stats # => StatCounter\r\n\r\nstats.min\r\nstats.max\r\nstats.count\r\nstats.mean\r\nstats.stdev\r\nstats.variance\r\nstats.sample_stdev\r\nstats.sample_variance\r\n```\r\n\r\nGithub: https://github.com/ondra-m/ruby-spark","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}