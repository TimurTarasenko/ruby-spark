{"name":"Ruby-spark","tagline":"Ruby wrapper for Apache Spark","body":"In this post, I describe how to parallelize computations in Ruby with **ruby-spark** gem. This library uses a Apache Spark project to storing and distributing data collections across cluster.\r\n\r\nRequirments:\r\n- Java 7+\r\n- Ruby 2+\r\n- MRI or JRuby\r\n\r\nGlossary:\r\n- Context: entry point for using Spark functionality\r\n- RDD: Resilient Distributed Dataset\r\n- Driver: a driver Spark instance (exist only once)\r\n- Executor: worker instance\r\n\r\n![Apache Spark cluster](http://spark.apache.org/docs/latest/img/cluster-overview.png)\r\n\r\n## Installation\r\n\r\n```bash\r\n# Install gem\r\ngem install ruby-spark\r\n\r\n# Build Spark and extensions (could take a while)\r\nruby-spark build\r\n\r\n# Set JAVA_HOME (required for MRI)\r\nexport JAVA_HOME=\"...\"\r\n```\r\n\r\n## Starting and configurations\r\n\r\nFor all setup options, please look on [wiki](https://github.com/ondra-m/ruby-spark/wiki/Configuration). All necessary configuration are set by default but if you want change it you need set keys before creating context. After that is configuration read-only.\r\n\r\n```ruby\r\nrequire 'ruby-spark'\r\n\r\n# Configuration\r\nSpark.config do\r\n  set_app_name 'My RubySpark'\r\n  set_master   'local[*]'\r\n  set 'spark.ruby.batch_size', 2048\r\nend\r\n\r\n# Create a context\r\nSpark.start\r\n\r\n# Context reference\r\nsc = Spark.sc\r\n```\r\n\r\nYou can also start prepared console by `ruby-spark shell`. This command will load RubySpark and create Pry console.\r\n\r\n## Usage\r\n\r\nFirst, you need create a distributed data collection. This dataset will be splitted into computing process. All process have the same computing function and cannot comunicate with each other.\r\n\r\n```ruby\r\nworker_nums = 2\r\nrands = Array.new(1000){ rand(1..10) }\r\n\r\nrdd_numbers = sc.parallelize(1..1000, worker_nums)\r\nrdd_rands = sc.parallelize(rands, worker_nums)\r\ntext_file = sc.text_file('/etc/hosts', worker_nums)\r\n```\r\n\r\n### Custom serializer\r\n\r\nRDD is using serializer defined from confing options (`spark.ruby-serializer*`). However if you want a different serializer just for one RDD you can do:\r\n\r\n```ruby\r\nser = Spark::Serializer.build { auto_batched(compressed(oj)) }\r\ncustom_rdd = sc.parallelize(1..1000, worker_nums, ser)\r\n```\r\n\r\n### Examples\r\n\r\nNow you can define a computing function. All function can be found at [ruby-doc](http://www.rubydoc.info/github/ondra-m/ruby-spark/Spark/RDD). Every new function is attached to dataset and are executed at once by `.collect` (lazy definition). However some methods start calculation automatically (e.g. sum, count, first, ...).\r\n\r\n\r\n#### Simple mapping\r\n\r\nThis function will be applied to every element in the collection.\r\n\r\n```ruby\r\nrdd_x2 = rdd_numbers.map(lambda{|x| x*2})\r\n\r\nrdd_x2.collect # => [2, 4, 6, 8, 10, 12, ...]\r\n```\r\n\r\n#### Pipelined functions\r\n\r\nYou can also add new function to old RDD.\r\n\r\n```ruby\r\nfiltered = rdd_x2.filter(lambda{|x| x%3 == 0})\r\n\r\nfiltered.collect # => [6, 12, 18, 24, 30, 36, ...]\r\n```\r\n\r\n#### Word count\r\n\r\nWord counting on text file.\r\n\r\n##### Version 1\r\n\r\n```\r\n# text_file: element on the collection is one line on the file\r\n\r\n# Split line to words\r\nwords = text_file.flat_map(:split)\r\n\r\n# Transform all word to [word, 1] (key, value)\r\narrays = words.map(lambda{|word| [word, 1]})\r\n\r\n# Merge words (values will be reduced)\r\ncount = arrays.reduce_by_key(lambda{|a, b| a+b})\r\n\r\ncount.collect # => [[\"127.0.0.1\", 1], [\"localhost\", 1], [\"#\", 3], ...]\r\n```\r\n\r\n##### Version 2\r\n\r\n```ruby\r\nword_count = lambda do |iterator|\r\n  result = Hash.new {|hash, key| hash[key] = 0}\r\n\r\n  iterator.each do |line|\r\n    line.split.each do |word|\r\n      result[word] += 1\r\n    end\r\n  end\r\n\r\n  result.to_a\r\nend\r\n\r\nreduce = lambda do |iterator|\r\n  result = Hash.new {|hash, key| hash[key] = 0}\r\n\r\n  iterator.each do |(word, count)|\r\n    result[word] += count\r\n  end\r\n\r\n  result.to_a\r\nend\r\n\r\n# Every node calculate word count on own collection\r\nrdd = text_file.map_partitions(word_count)\r\n\r\n# Set worker count to 1\r\nrdd = rdd.coalesce(1)\r\n\r\n# Reduce all prev results\r\nrdd = rdd.map_partitions(reduce)\r\n\r\nrdd.collect # => [[\"127.0.0.1\", 1], [\"localhost\", 1], [\"#\", 3], ...]\r\n```\r\n\r\n#### Estimating PI\r\n\r\nUsing Ruby Math library.\r\n\r\n```ruby\r\nrdd = sc.parallelize([10_000], 1)\r\nrdd = rdd.add_library('bigdecimal/math')\r\nrdd = rdd.map(lambda{|x| BigMath.PI(x)})\r\nrdd.collect # => #<BigDecimal, '0.31415926...'>\r\n```\r\n\r\n#### Basic statistic\r\n\r\n```\r\n# Stats\r\nrdd = rdd_numbers.map(lambda{|x| (x * rand) ** 2})\r\nstats = rdd.stats # => StatCounter\r\n\r\nstats.min\r\nstats.max\r\nstats.count\r\nstats.mean\r\nstats.stdev\r\nstats.variance\r\nstats.sample_stdev\r\nstats.sample_variance\r\n```\r\n\r\n#### Linear regression\r\n\r\nMllib functions are using Spark's Machine Learning Library.\r\n\r\n```ruby\r\n# Import Mllib classes to Object\r\nSpark::Mllib.import\r\n\r\n# Dense vectors\r\ndata = [\r\n  LabeledPoint.new(0.0, [0.0]),\r\n  LabeledPoint.new(1.0, [1.0]),\r\n  LabeledPoint.new(3.0, [2.0]),\r\n  LabeledPoint.new(2.0, [3.0])\r\n]\r\nlrm = LinearRegressionWithSGD.train(sc.parallelize(data), initial_weights: [1.0])\r\n\r\nlrm.intercept # => 0.0\r\nlrm.weights   # => [0.9285714285714286]\r\n\r\nlrm.predict([0.0]) # => 0.0\r\nlrm.predict([0.7]) # => 0.65\r\nlrm.predict([0.6]) # => 0.5571428571428572\r\n```\r\n\r\n## Computing model\r\n\r\n<img src=\"http://i.imgur.com/vhIrosW.png\" >\r\n\r\n\r\n## Benchmarks\r\n\r\nAll benchmarks can be found on [github repo](https://github.com/ondra-m/ruby-spark/tree/master/benchmark/comparison).\r\n\r\n### Serializations\r\n\r\n#### Integers\r\n\r\n<img src=\"http://i.imgur.com/zEZQpGp.png\">\r\n\r\n#### Floats\r\n\r\n<img src=\"http://i.imgur.com/GiCKDyx.png\">\r\n\r\n#### Text\r\n\r\n<img src=\"http://i.imgur.com/itgwSSQ.png\">\r\n\r\n### Coputing\r\n\r\n#### Prime number\r\n\r\nCheck if number is prime\r\n\r\n<img src=\"http://i.imgur.com/6TRSpTI.png\">\r\n\r\n#### Matrix multiplication\r\n\r\nSquare matrix multiplication. Matrix is represented by build Array in every language.\r\n\r\n<img src=\"http://i.imgur.com/ftJ44M2.png\">\r\n\r\n#### PI digits\r\n\r\nComputing PI number to X digit. Algorithm is borrowed from http://rosettacode.org/wiki/Pi.\r\n\r\n<img src=\"http://i.imgur.com/YX6tPPn.png\">\r\n\r\n\r\nGithub: https://github.com/ondra-m/ruby-spark\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}